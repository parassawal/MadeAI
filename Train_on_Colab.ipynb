{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Python Code Generator - Training on Google Colab\n",
                "\n",
                "This notebook trains a **CodeLlama-7B** model on your Python code dataset using a **free T4 GPU**.\n",
                "\n",
                "## Setup Instructions:\n",
                "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí T4 GPU ‚Üí Save\n",
                "2. **Run all cells** in order\n",
                "3. **Upload** `python-codes-25k.json` when prompted\n",
                "4. **Wait** 2-4 hours for training\n",
                "5. **Download** the trained model\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Check GPU Availability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Install Dependencies\n",
                "\n",
                "This will take about 5 minutes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install -q transformers datasets accelerate peft trl\n",
                "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "\n",
                "print(\"‚úÖ Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Upload Dataset\n",
                "\n",
                "Click the \"Choose Files\" button and select `python-codes-25k.json`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "print(\"üìÇ Upload your python-codes-25k.json file:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "print(\"\\n‚úÖ Dataset uploaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Prepare Data\n",
                "\n",
                "Process the dataset into training format."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import pandas as pd\n",
                "from datasets import Dataset, DatasetDict\n",
                "import re\n",
                "\n",
                "def extract_code_from_output(output: str) -> str:\n",
                "    \"\"\"Extract Python code from markdown code blocks\"\"\"\n",
                "    code_match = re.search(r'```python\\n(.*?)\\n```', output, re.DOTALL)\n",
                "    if code_match:\n",
                "        return code_match.group(1)\n",
                "    return output\n",
                "\n",
                "def format_instruction(instruction: str, output: str) -> dict:\n",
                "    \"\"\"Format data in instruction-following format\"\"\"\n",
                "    code = extract_code_from_output(output)\n",
                "    \n",
                "    prompt = f\"\"\"### Instruction:\n",
                "{instruction.strip()}\n",
                "\n",
                "### Response:\n",
                "```python\n",
                "{code.strip()}\n",
                "```\"\"\"\n",
                "    \n",
                "    return {\n",
                "        \"text\": prompt,\n",
                "        \"instruction\": instruction.strip(),\n",
                "        \"output\": code.strip()\n",
                "    }\n",
                "\n",
                "# Load and process data\n",
                "print(\"Loading dataset...\")\n",
                "with open('python-codes-25k.json', 'r', encoding='utf-8') as f:\n",
                "    data = json.load(f)\n",
                "\n",
                "print(f\"Loaded {len(data)} examples\")\n",
                "\n",
                "# Format examples\n",
                "formatted_data = []\n",
                "for item in data:\n",
                "    if 'instruction' in item and 'output' in item:\n",
                "        formatted_data.append(format_instruction(item['instruction'], item['output']))\n",
                "\n",
                "print(f\"Formatted {len(formatted_data)} examples\")\n",
                "\n",
                "# Create dataset\n",
                "df = pd.DataFrame(formatted_data)\n",
                "dataset = Dataset.from_pandas(df)\n",
                "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
                "\n",
                "dataset_dict = DatasetDict({\n",
                "    'train': split_dataset['train'],\n",
                "    'validation': split_dataset['test']\n",
                "})\n",
                "\n",
                "print(f\"\\n‚úÖ Data prepared!\")\n",
                "print(f\"   Train: {len(dataset_dict['train'])} examples\")\n",
                "print(f\"   Val: {len(dataset_dict['validation'])} examples\")\n",
                "\n",
                "# Show sample\n",
                "print(\"\\n--- Sample Example ---\")\n",
                "print(dataset_dict['train'][0]['text'][:500] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Load Model and Configure LoRA\n",
                "\n",
                "Load CodeLlama-7B with efficient 4-bit quantization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "\n",
                "# Model configuration\n",
                "MODEL_NAME = \"codellama/CodeLlama-7b-hf\"\n",
                "MAX_SEQ_LENGTH = 2048\n",
                "\n",
                "print(\"Loading CodeLlama-7B model...\")\n",
                "print(\"This will download ~13GB, may take 5-10 minutes.\\n\")\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=MODEL_NAME,\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    dtype=None,\n",
                "    load_in_4bit=True,\n",
                ")\n",
                "\n",
                "# Add LoRA adapters\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r=16,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
                "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha=32,\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    use_gradient_checkpointing=True,\n",
                "    random_state=42,\n",
                ")\n",
                "\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "print(\"\\n‚úÖ Model loaded!\")\n",
                "print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Train the Model\n",
                "\n",
                "**This will take 2-4 hours on a T4 GPU.**\n",
                "\n",
                "You can monitor progress below. The loss should decrease over time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import TrainingArguments\n",
                "from trl import SFTTrainer\n",
                "\n",
                "# Training configuration\n",
                "OUTPUT_DIR = \"pythoncode-lora\"\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=4,\n",
                "    per_device_eval_batch_size=4,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    warmup_steps=100,\n",
                "    logging_steps=10,\n",
                "    save_steps=500,\n",
                "    eval_steps=500,\n",
                "    evaluation_strategy=\"steps\",\n",
                "    save_strategy=\"steps\",\n",
                "    load_best_model_at_end=True,\n",
                "    fp16=True,\n",
                "    optim=\"adamw_8bit\",\n",
                "    weight_decay=0.01,\n",
                "    max_grad_norm=1.0,\n",
                "    report_to=\"none\",\n",
                "    save_total_limit=3,\n",
                ")\n",
                "\n",
                "# Create trainer\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    train_dataset=dataset_dict['train'],\n",
                "    eval_dataset=dataset_dict['validation'],\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    args=training_args,\n",
                "    packing=False,\n",
                ")\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"üöÄ Starting Training...\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Total steps: {len(dataset_dict['train']) // 16 * 3}\")\n",
                "print(f\"This will take approximately 2-4 hours.\\n\")\n",
                "\n",
                "# Start training\n",
                "trainer.train()\n",
                "\n",
                "# Save final model\n",
                "trainer.save_model(OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"‚úÖ Training Complete!\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Test the Model\n",
                "\n",
                "Let's test with a few examples before downloading."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set model to inference mode\n",
                "model = FastLanguageModel.for_inference(model)\n",
                "\n",
                "def generate_code(instruction: str):\n",
                "    prompt = f\"\"\"### Instruction:\n",
                "{instruction}\n",
                "\n",
                "### Response:\n",
                "\"\"\"\n",
                "    \n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "    \n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=512,\n",
                "        temperature=0.7,\n",
                "        top_p=0.9,\n",
                "        do_sample=True,\n",
                "    )\n",
                "    \n",
                "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    \n",
                "    # Extract only the response\n",
                "    if \"### Response:\" in result:\n",
                "        return result.split(\"### Response:\")[1].strip()\n",
                "    return result\n",
                "\n",
                "# Test examples\n",
                "test_prompts = [\n",
                "    \"Create a function to calculate factorial of a number\",\n",
                "    \"Write a function to check if a string is a palindrome\",\n",
                "    \"Implement binary search algorithm\",\n",
                "]\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"Testing Model\")\n",
                "print(\"=\"*50 + \"\\n\")\n",
                "\n",
                "for prompt in test_prompts:\n",
                "    print(f\"üìù Instruction: {prompt}\")\n",
                "    print(\"\\nüíª Generated Code:\")\n",
                "    print(generate_code(prompt))\n",
                "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Download the Trained Model\n",
                "\n",
                "Download the model to use on your Mac with Ollama."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "from google.colab import files\n",
                "\n",
                "print(\"Creating ZIP file...\")\n",
                "shutil.make_archive('pythoncode-lora', 'zip', 'pythoncode-lora')\n",
                "\n",
                "print(\"Downloading model... (this may take a few minutes)\")\n",
                "files.download('pythoncode-lora.zip')\n",
                "\n",
                "print(\"\\n‚úÖ Download complete!\")\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"1. Extract the ZIP file on your Mac\")\n",
                "print(\"2. Run: python export_to_ollama.py\")\n",
                "print(\"3. Run: ollama create pythoncode -f Modelfile\")\n",
                "print(\"4. Run: ollama run pythoncode\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéâ All Done!\n",
                "\n",
                "Your model is trained and ready to use!\n",
                "\n",
                "### What You Have:\n",
                "- ‚úÖ Fine-tuned CodeLlama-7B model\n",
                "- ‚úÖ Trained on 25K Python code examples  \n",
                "- ‚úÖ Ready to deploy with Ollama\n",
                "\n",
                "### Usage on Your Mac:\n",
                "```bash\n",
                "# Extract the downloaded ZIP\n",
                "unzip pythoncode-lora.zip\n",
                "\n",
                "# Export to Ollama format\n",
                "python export_to_ollama.py\n",
                "\n",
                "# Create Ollama model\n",
                "ollama create pythoncode -f Modelfile\n",
                "\n",
                "# Use it!\n",
                "ollama run pythoncode \"Create a function to reverse a string\"\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}